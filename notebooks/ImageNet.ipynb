{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750a9ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:01:19.240287Z",
     "start_time": "2021-07-28T16:01:18.122480Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, GPUStatsMonitor, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pl_bolts.datamodules.cifar10_datamodule import CIFAR10DataModule\n",
    "from pl_bolts.datamodules.imagenet_datamodule import ImagenetDataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization, imagenet_normalization\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b361cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:05:08.909819Z",
     "start_time": "2021-07-28T16:05:04.444960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "class Transform:\n",
    "    def __init__(self, transform: A.Compose):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img, *args, **kwargs):\n",
    "        return self.transform(image=np.array(img), *args, **kwargs)\n",
    "\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 30\n",
    "data_folder = \"/home/dima/datasets/imagenet/\"\n",
    "\n",
    "dm = ImagenetDataModule(data_folder, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dm.train_transforms = Transform(A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    ToTensorV2(),\n",
    "]))\n",
    "dm.val_transforms = Transform(A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "]))\n",
    "dm.setup()\n",
    "\n",
    "STEPS_PER_EPOCH = len(dm.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51890734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:25:07.031573Z",
     "start_time": "2021-07-28T16:25:07.023450Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=768, num_channels=3, norm_layer=None, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size = (img_size, img_size)\n",
    "        self.patch_size = patch_size = (patch_size, patch_size)\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(num_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(emb_dim) if norm_layer else nn.Identity()\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, 1 + self.num_patches, emb_dim))  # + cls_token\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        x = self.norm(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.dropout(x + self.pos_embedding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc12ce56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:25:07.551856Z",
     "start_time": "2021-07-28T16:25:07.540150Z"
    }
   },
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=512, depth=6, num_heads=8, \n",
    "                 num_channels=3, num_classes=10, dropout=0.1, emb_dropout=0.1, \n",
    "                 lr=1e-3, weight_decay=0, warmup=0, max_iters=2000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.embedding = PatchEmbedding(img_size, patch_size, emb_dim, num_channels, dropout=emb_dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(emb_dim, num_heads, dropout=dropout, \n",
    "                                                   activation='gelu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, depth)\n",
    "        self.norm = nn.LayerNorm(emb_dim, eps=1e-6)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "        \n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.val_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x['image'])\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc(y_pred, y), on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x['image'])\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc(y_pred, y), on_step=False, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x['image'])\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', self.test_acc(y_pred, y), on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(optimizer, self.hparams.warmup, self.hparams.max_iters)\n",
    "        scheduler_dict = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12c456",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-28T16:25:13.400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | embedding   | PatchEmbedding     | 742 K \n",
      "1 | transformer | TransformerEncoder | 44.1 M\n",
      "2 | norm        | LayerNorm          | 1.5 K \n",
      "3 | classifier  | Linear             | 769 K \n",
      "4 | train_acc   | Accuracy           | 0     \n",
      "5 | val_acc     | Accuracy           | 0     \n",
      "6 | test_acc    | Accuracy           | 0     \n",
      "---------------------------------------------------\n",
      "45.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "45.6 M    Total params\n",
      "182.500   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad4e579104848a8a2088224bb2da7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ViT(img_size=224, patch_size=16, lr=1e-3, weight_decay=0.1, \n",
    "            emb_dim=768, depth=8, num_heads=12, num_classes=1000, \n",
    "            warmup=2*STEPS_PER_EPOCH, max_iters=NUM_EPOCHS*STEPS_PER_EPOCH)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor='val_loss', \n",
    "    filename='imagenet-{epoch}-{val_loss:.3f}', \n",
    "    dirpath='/home/dima/ViTransformer/checkpoints/', \n",
    "    mode='min',\n",
    ")\n",
    "gpu_monitor = GPUStatsMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    gpus=1,\n",
    "    logger=TensorBoardLogger('/home/dima/lightning_logs/', name='vit_imagenet'),\n",
    "#     default_root_dir='/home/dima/vitransformer/checkpoints/',\n",
    "    callbacks=[lr_monitor, model_checkpoint, gpu_monitor],\n",
    "    gradient_clip_val=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2e1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
