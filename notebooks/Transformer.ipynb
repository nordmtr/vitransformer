{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69bed1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:04:03.720795Z",
     "start_time": "2021-07-28T16:04:02.576584Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, GPUStatsMonitor, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pl_bolts.datamodules.cifar10_datamodule import CIFAR10DataModule\n",
    "from pl_bolts.datamodules.imagenet_datamodule import ImagenetDataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization, imagenet_normalization\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74071f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:11:58.870470Z",
     "start_time": "2021-07-28T16:11:57.950535Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "class Transform:\n",
    "    def __init__(self, transform: A.Compose):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img, *args, **kwargs):\n",
    "        return self.transform(image=np.array(img), *args, **kwargs)\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pl.seed_everything(42)\n",
    "\n",
    "batch_size = 32\n",
    "data_folder = \"/home/dima/datasets/CIFAR10\"\n",
    "\n",
    "dm = CIFAR10DataModule(data_folder, batch_size=batch_size, shuffle=True)\n",
    "dm.train_transforms = Transform(A.Compose([\n",
    "    A.Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.244, 0.262)),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    ToTensorV2(),\n",
    "]))\n",
    "dm.val_transforms = Transform(A.Compose([\n",
    "    A.Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.244, 0.262)),\n",
    "    ToTensorV2(),\n",
    "]))\n",
    "dm.test_transforms = Transform(A.Compose([\n",
    "    A.Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.244, 0.262)),\n",
    "    ToTensorV2(),\n",
    "]))\n",
    "dm.setup()\n",
    "\n",
    "steps_per_epoch = len(dm.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59df1358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:11:58.880382Z",
     "start_time": "2021-07-28T16:11:58.871772Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=768, num_channels=3, norm_layer=None, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size = (img_size, img_size)\n",
    "        self.patch_size = patch_size = (patch_size, patch_size)\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(num_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(emb_dim) if norm_layer else nn.Identity()\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, 1 + self.num_patches, emb_dim))  # + cls_token\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        x = self.norm(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.dropout(x + self.pos_embedding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0ecf0c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:18:45.384289Z",
     "start_time": "2021-07-28T16:18:45.372393Z"
    }
   },
   "outputs": [],
   "source": [
    "class VanillaViT(pl.LightningModule):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=512, depth=6, num_heads=8, \n",
    "                 num_channels=3, num_classes=10, dropout=0.1, emb_dropout=0.1, \n",
    "                 lr=1e-3, weight_decay=0, warmup=2*steps_per_epoch, max_iters=30*steps_per_epoch):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.embedding = PatchEmbedding(img_size, patch_size, emb_dim, num_channels, dropout=emb_dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(emb_dim, num_heads, dropout=dropout, \n",
    "                                                   activation='gelu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, depth)\n",
    "        self.norm = nn.LayerNorm(emb_dim, eps=1e-6)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "        \n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.val_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x['image'])\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc(y_pred, y), on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x['image'])\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc(y_pred, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x['image'])\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', self.test_acc(y_pred, y), on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(optimizer, self.hparams.warmup, self.hparams.max_iters)\n",
    "        scheduler_dict = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b95dfbcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T16:20:03.969175Z",
     "start_time": "2021-07-28T16:18:50.289292Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | embedding   | PatchEmbedding     | 108 K \n",
      "1 | transformer | TransformerEncoder | 18.9 M\n",
      "2 | norm        | LayerNorm          | 1.0 K \n",
      "3 | classifier  | Linear             | 5.1 K \n",
      "4 | train_acc   | Accuracy           | 0     \n",
      "5 | val_acc     | Accuracy           | 0     \n",
      "6 | test_acc    | Accuracy           | 0     \n",
      "---------------------------------------------------\n",
      "19.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "19.0 M    Total params\n",
      "76.114    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31930bf44fe4df699f70ce23f0acebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/.cache/pypoetry/virtualenvs/vitransformer-feYZhwM1-py3.8/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "model = VanillaViT(img_size=32, patch_size=8, lr=1e-4)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor='val_loss', \n",
    "    filename='cifar10-{epoch}-{val_loss:.3f}', \n",
    "    dirpath='/home/dima/ViTransformer/checkpoints/', \n",
    "    mode='min',\n",
    ")\n",
    "gpu_monitor = GPUStatsMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    gpus=1,\n",
    "    logger=TensorBoardLogger('/home/dima/lightning_logs/', name='vanilla_vit_cifar10'),\n",
    "#     default_root_dir='/home/dima/vitransformer/checkpoints/',\n",
    "    callbacks=[lr_monitor, model_checkpoint, gpu_monitor],\n",
    "    gradient_clip_val=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b37966a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T15:51:40.122755Z",
     "start_time": "2021-07-28T15:51:40.118525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dima/ViTransformer/checkpoints/cifar10-epoch=0-val_loss=1.841.ckpt'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d383f6e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T15:49:06.744068Z",
     "start_time": "2021-07-28T15:49:06.580061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaViT(\n",
       "  (embedding): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 512, kernel_size=(8, 8), stride=(8, 8))\n",
       "    (norm): Identity()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (train_acc): Accuracy()\n",
       "  (val_acc): Accuracy()\n",
       "  (test_acc): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_from_checkpoint(model_checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d915322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T13:49:06.588784Z",
     "start_time": "2021-07-28T13:49:06.482259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = nn.Parameter(torch.empty(4,4))\n",
    "optimizer = optim.Adam([p], lr=1e-3)\n",
    "lr_scheduler = LinearWarmupCosineAnnealingLR(optimizer, 100, 2000)\n",
    "# lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
    "\n",
    "# Plotting\n",
    "epochs = list(range(4000))\n",
    "lr = []\n",
    "for _ in range(4000):\n",
    "    lr.append(lr_scheduler.get_lr())\n",
    "    lr_scheduler.step()\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(epochs, lr)\n",
    "plt.ylabel(\"Learning rate factor\")\n",
    "plt.xlabel(\"Iterations (in batches)\")\n",
    "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f219e71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T19:39:13.338974Z",
     "start_time": "2021-07-27T19:39:13.326186Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, num_channels=3, emb_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size\"\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "74f19575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T19:35:39.412491Z",
     "start_time": "2021-07-27T19:35:39.406029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "patch_embed = PatchEmbed(img_size=32, patch_size=8, embed_dim=512)\n",
    "patches = patch_embed(images[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec54fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n",
    "        - https://arxiv.org/abs/2012.12877\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None, weight_init=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            weight_init: (str): weight init scheme\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.dist_token is not None:\n",
    "            trunc_normal_(self.dist_token, std=.02)\n",
    "        if mode.startswith('jax'):\n",
    "            # leave cls token as zeros to match jax impl\n",
    "            named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)\n",
    "        else:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "            self.apply(_init_vit_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        _init_vit_weights(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        if self.dist_token is None:\n",
    "            return self.head\n",
    "        else:\n",
    "            return self.head, self.head_dist\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        if self.num_tokens == 2:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):\n",
    "    \"\"\" ViT weight initialization\n",
    "    * When called without n, head_bias, jax_impl args it will behave exactly the same\n",
    "      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n",
    "    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if name.startswith('head'):\n",
    "            nn.init.zeros_(module.weight)\n",
    "            nn.init.constant_(module.bias, head_bias)\n",
    "        elif name.startswith('pre_logits'):\n",
    "            lecun_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            if jax_impl:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    if 'mlp' in name:\n",
    "                        nn.init.normal_(module.bias, std=1e-6)\n",
    "                    else:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "            else:\n",
    "                trunc_normal_(module.weight, std=.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    elif jax_impl and isinstance(module, nn.Conv2d):\n",
    "        # NOTE conv was left to pytorch default in my original init\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
