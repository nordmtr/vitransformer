{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c221e8b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T14:14:08.110943Z",
     "start_time": "2021-07-30T14:14:07.513660Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, GPUStatsMonitor, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from pl_bolts.datamodules.cifar10_datamodule import CIFAR10DataModule\n",
    "from pl_bolts.datamodules.imagenet_datamodule import ImagenetDataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization, imagenet_normalization\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479e6545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T14:14:16.111095Z",
     "start_time": "2021-07-30T14:14:11.725551Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "class Transform:\n",
    "    def __init__(self, transform: A.Compose):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img, *args, **kwargs):\n",
    "        return self.transform(image=np.array(img), *args, **kwargs)['image']\n",
    "\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "data_folder = \"/home/dima/datasets/imagenet/\"\n",
    "\n",
    "dm = ImagenetDataModule(data_folder, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "# dm.train_transforms = Transform(A.Compose([\n",
    "#     A.Resize(224, 224),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomResizedCrop(224, 224, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "#     ToTensorV2(),\n",
    "# ]))\n",
    "# dm.val_transforms = Transform(A.Compose([\n",
    "#     A.Resize(224, 224),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#     ToTensorV2(),\n",
    "# ]))\n",
    "dm.train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop((224, 224),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "dm.val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "dm.setup()\n",
    "\n",
    "STEPS_PER_EPOCH = len(dm.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08a1d74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T14:17:20.967974Z",
     "start_time": "2021-07-30T14:17:20.951078Z"
    },
    "code_folding": [
     0,
     30,
     47
    ]
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=768, num_channels=3, norm_layer=None, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size = (img_size, img_size)\n",
    "        self.patch_size = patch_size = (patch_size, patch_size)\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(num_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(emb_dim) if norm_layer else nn.Identity()\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, 1 + self.num_patches, emb_dim))  # + cls_token\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        x = self.norm(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.dropout(x + self.pos_embedding)\n",
    "        return x\n",
    "    \n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=0):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, hidden_dim=2048, dropout=0., drop_path=0.):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            emb_dim - Dimensionality of input and attention feature vectors\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than emb_dim)\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(emb_dim)\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, batch_first=True)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.layer_norm_2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.drop_path(self.attn(inp_x, inp_x, inp_x)[0])\n",
    "        x = x + self.drop_path(self.mlp(self.layer_norm_2(x)))\n",
    "        return x\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=512, mlp_dim=2048, depth=6, num_heads=8, \n",
    "                 num_channels=3, num_classes=10, dropout=0.1, emb_dropout=0.1, drop_path=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = PatchEmbedding(img_size, patch_size, emb_dim, num_channels, dropout=emb_dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, depth)]\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            AttentionBlock(emb_dim, num_heads, mlp_dim, dropout=dropout, drop_path=dpr[i]) for i in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(emb_dim, eps=1e-6)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c11fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T14:17:44.515310Z",
     "start_time": "2021-07-30T14:17:44.504538Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3, weight_decay=0, max_iters=2000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters(ignore='model')\n",
    "        self.model = model\n",
    "        \n",
    "        self.train_acc_top1 = Accuracy()\n",
    "        self.val_acc_top1 = Accuracy()\n",
    "        self.test_acc_top1 = Accuracy()\n",
    "        \n",
    "        self.train_acc_top5 = Accuracy(top_k=5)\n",
    "        self.val_acc_top5 = Accuracy(top_k=5)\n",
    "        self.test_acc_top5 = Accuracy(top_k=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc_top1', self.train_acc_top1(y_pred, y), on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.log('train_acc_top5', self.train_acc_top5(y_pred, y), on_step=True, on_epoch=False, prog_bar=False)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc_top1', self.val_acc_top1(y_pred, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc_top5', self.val_acc_top5(y_pred, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc_top1', self.test_acc_top1(y_pred, y), on_step=False, on_epoch=True)\n",
    "        self.log('test_acc_top5', self.test_acc_top5(y_pred, y), on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        scheduler = CosineAnnealingLR(optimizer, self.hparams.max_iters, eta_min=self.hparams.lr*1e-2)\n",
    "        scheduler_dict = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a090077",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-30T14:18:54.311Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/.cache/pypoetry/virtualenvs/vitransformer-feYZhwM1-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /home/dima/ViTransformer/checkpoints/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/dima/.cache/pypoetry/virtualenvs/vitransformer-feYZhwM1-py3.8/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type              | Params\n",
      "-----------------------------------------------------\n",
      "0 | model          | VisionTransformer | 45.6 M\n",
      "1 | train_acc_top1 | Accuracy          | 0     \n",
      "2 | val_acc_top1   | Accuracy          | 0     \n",
      "3 | test_acc_top1  | Accuracy          | 0     \n",
      "4 | train_acc_top5 | Accuracy          | 0     \n",
      "5 | val_acc_top5   | Accuracy          | 0     \n",
      "6 | test_acc_top5  | Accuracy          | 0     \n",
      "-----------------------------------------------------\n",
      "45.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "45.6 M    Total params\n",
      "182.500   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147c167cbb4549fb9eb81528759c7084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "model = VisionTransformer(img_size=224, patch_size=16, emb_dim=768, \n",
    "                          depth=8, num_heads=12, num_classes=1000, drop_path=0.3)\n",
    "vit = ViT(model, lr=1e-4, weight_decay=1e-5, max_iters=NUM_EPOCHS*STEPS_PER_EPOCH)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor='val_loss', \n",
    "    filename='imagenetv3-{epoch}-{val_loss:.3f}', \n",
    "    dirpath='/home/dima/ViTransformer/checkpoints/', \n",
    "    mode='min',\n",
    ")\n",
    "gpu_monitor = GPUStatsMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    gpus=1,\n",
    "    logger=TensorBoardLogger('/home/dima/lightning_logs/', name='vit_imagenet'),\n",
    "#     default_root_dir='/home/dima/vitransformer/checkpoints/',\n",
    "    callbacks=[lr_monitor, model_checkpoint, gpu_monitor],\n",
    "    gradient_clip_val=1,\n",
    ")\n",
    "\n",
    "trainer.fit(vit, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf29803",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    gpus=1,\n",
    "    logger=TensorBoardLogger('/home/dima/lightning_logs/', name='vit_imagenet'),\n",
    "#     default_root_dir='/home/dima/vitransformer/checkpoints/',\n",
    "    callbacks=[lr_monitor, model_checkpoint, gpu_monitor],\n",
    "    gradient_clip_val=1,\n",
    ")\n",
    "\n",
    "trainer.fit(vit, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a954f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
