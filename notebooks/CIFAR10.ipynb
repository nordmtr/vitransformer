{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69bed1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T11:03:58.732414Z",
     "start_time": "2021-07-29T11:03:57.606795Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, GPUStatsMonitor, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pl_bolts.datamodules.cifar10_datamodule import CIFAR10DataModule\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5426cbb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T10:40:31.431505Z",
     "start_time": "2021-07-29T10:40:31.425868Z"
    },
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "class RandAugment:\n",
    "    def __init__(self, N, M, p=0.5, mode=\"all\"):\n",
    "        self.N = N\n",
    "        self.mode = mode\n",
    "        # Magnitude(M) search space\n",
    "        shift_x = np.linspace(0, 0.3, 10)\n",
    "        shift_y = np.linspace(0, 0.3, 10)\n",
    "        rot = np.linspace(0, 30, 10)\n",
    "        shear = np.linspace(0, 20, 10)\n",
    "        sola = np.linspace(0, 256, 10)\n",
    "        cont = [np.linspace(-0.8, -0.1, 10),np.linspace(0.1, 2, 10)]\n",
    "        bright = np.linspace(0.1, 0.7, 10)\n",
    "        shar = np.linspace(0.1, 0.9, 10)\n",
    "        cut = np.linspace(0, 60, 10)\n",
    "        # Transformation search space\n",
    "        self.augs = [#0 - geometrical\n",
    "            A.ShiftScaleRotate(shift_limit_x=shift_x[M], rotate_limit=0, shift_limit_y=0, p=p),\n",
    "            A.ShiftScaleRotate(shift_limit_y=shift_y[M], rotate_limit=0, shift_limit_x=0, p=p),\n",
    "            A.Affine(rotate=rot[M], p=p),\n",
    "            A.Affine(shear=shear[M], p=p),\n",
    "            A.InvertImg(p=p),\n",
    "            #5 - Color Based\n",
    "            A.Equalize(p=p),\n",
    "            A.Solarize(threshold=sola[M], p=p),\n",
    "            A.RandomBrightnessContrast(contrast_limit=(cont[0][M], cont[1][M]), brightness_limit=0, p=p),\n",
    "            A.RandomBrightnessContrast(brightness_limit=bright[M], contrast_limit=0, p=p),\n",
    "            A.Sharpen(alpha=shar[M], lightness=shar[M], p=p)\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, img, *args, **kwargs):\n",
    "        # Sampling from the Transformation search space\n",
    "        if self.mode == \"geo\":\n",
    "            ops = np.random.choice(self.augs[0:5], self.N)\n",
    "        elif self.mode == \"color\":\n",
    "            ops = np.random.choice(self.augs[5:], self.N)\n",
    "        else:\n",
    "            ops = np.random.choice(self.augs, self.N)\n",
    "        transforms = A.Compose(ops)\n",
    "        return transforms(image=np.array(img), *args, **kwargs)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4cffe97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T10:45:53.875213Z",
     "start_time": "2021-07-29T10:45:52.884239Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "data_folder = \"/home/dima/datasets/CIFAR10\"\n",
    "\n",
    "dm = CIFAR10DataModule(data_folder, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "# dm.train_transforms = Transform(A.Compose([\n",
    "#     rand_augment(2, 5, 0.5),\n",
    "#     A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "#     ToTensorV2(),\n",
    "# ]))\n",
    "dm.train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.244, 0.262)),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    RandAugment(N=2, M=3),\n",
    "#     transforms.ToTensor(),\n",
    "])\n",
    "dm.val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.244, 0.262)),\n",
    "])\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1976e1ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T12:23:30.716306Z",
     "start_time": "2021-07-29T12:23:29.761366Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "class Transform:\n",
    "    def __init__(self, transform: A.Compose):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img, *args, **kwargs):\n",
    "        return self.transform(image=np.array(img), *args, **kwargs)['image']\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pl.seed_everything(42)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "data_folder = \"/home/dima/datasets/CIFAR10\"\n",
    "\n",
    "p = 0.3\n",
    "\n",
    "dm = CIFAR10DataModule(data_folder, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "# dm.train_transforms = Transform(A.Compose([\n",
    "#     A.Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.244, 0.262)),\n",
    "#     A.HorizontalFlip(p=p),\n",
    "#     A.ShiftScaleRotate(p=p),\n",
    "#     A.RandomBrightnessContrast(p=p),\n",
    "#     A.RandomResizedCrop(32, 32, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "#     ToTensorV2(),\n",
    "# ]))\n",
    "# dm.val_transforms = Transform(A.Compose([\n",
    "#     A.Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.244, 0.262)),\n",
    "#     ToTensorV2(),\n",
    "# ]))\n",
    "dm.test_transforms = Transform(A.Compose([\n",
    "    A.Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.244, 0.262)),\n",
    "    ToTensorV2(),\n",
    "]))\n",
    "dm.train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "])\n",
    "dm.val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "])\n",
    "dm.setup()\n",
    "\n",
    "STEPS_PER_EPOCH = len(dm.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f4d0fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T12:23:47.080669Z",
     "start_time": "2021-07-29T12:23:47.066675Z"
    },
    "code_folding": [
     0,
     30,
     60
    ]
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=768, num_channels=3, norm_layer=None, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size = (img_size, img_size)\n",
    "        self.patch_size = patch_size = (patch_size, patch_size)\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(num_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(emb_dim) if norm_layer else nn.Identity()\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, 1 + self.num_patches, emb_dim))  # + cls_token\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        x = self.norm(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.dropout(x + self.pos_embedding)\n",
    "        return x\n",
    "        \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, hidden_dim=2048, dropout=0.):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            emb_dim - Dimensionality of input and attention feature vectors\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than emb_dim)\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(emb_dim)\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, batch_first=True)\n",
    "        self.layer_norm_2 = nn.LayerNorm(emb_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=512, depth=6, num_heads=8, \n",
    "                 num_channels=3, num_classes=10, dropout=0.1, emb_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = PatchEmbedding(img_size, patch_size, emb_dim, num_channels, dropout=emb_dropout)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            AttentionBlock(emb_dim, num_heads, dropout=dropout) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(emb_dim, eps=1e-6)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972a50ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T11:04:01.037903Z",
     "start_time": "2021-07-29T11:04:01.027587Z"
    },
    "code_folding": [
     0,
     5,
     13,
     26,
     56,
     71
    ]
   },
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class VisionTransformerRepo(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, \n",
    "                 pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1fc3d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T12:23:57.315477Z",
     "start_time": "2021-07-29T12:23:57.305911Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3, weight_decay=0, max_iters=2000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters(ignore='model')\n",
    "        self.model = model\n",
    "        \n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.val_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc(y_pred, y), on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc(y_pred, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', self.test_acc(y_pred, y), on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
    "#         return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        scheduler = CosineAnnealingLR(optimizer, self.hparams.max_iters, eta_min=self.hparams.lr*1e-2)\n",
    "        scheduler_dict = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d48750f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T12:54:31.351512Z",
     "start_time": "2021-07-29T12:24:00.378972Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | VisionTransformer | 19.0 M\n",
      "1 | train_acc | Accuracy          | 0     \n",
      "2 | val_acc   | Accuracy          | 0     \n",
      "3 | test_acc  | Accuracy          | 0     \n",
      "------------------------------------------------\n",
      "19.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "19.0 M    Total params\n",
      "75.917    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e600324a1f9246f0b64ec33c26e67891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 70\n",
    "\n",
    "model = VisionTransformer(img_size=32, patch_size=4)\n",
    "# model = VisionTransformerRepo(image_size=32, patch_size=4, num_classes=10, dim=512, depth=6, heads=8, mlp_dim=2048)\n",
    "vit = ViT(model, lr=1e-4, weight_decay=0., max_iters=NUM_EPOCHS*STEPS_PER_EPOCH)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor='val_loss', \n",
    "    filename='cifar10-{epoch}-{val_loss:.3f}', \n",
    "    dirpath='/home/dima/ViTransformer/checkpoints/', \n",
    "    mode='min',\n",
    ")\n",
    "gpu_monitor = GPUStatsMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    gpus=1,\n",
    "    logger=TensorBoardLogger('/home/dima/lightning_logs/', name='vit_cifar10_repo'),\n",
    "#     default_root_dir='/home/dima/vitransformer/checkpoints/',\n",
    "    callbacks=[lr_monitor, model_checkpoint, gpu_monitor],\n",
    "    gradient_clip_val=1,\n",
    ")\n",
    "\n",
    "trainer.fit(vit, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea75d806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T15:51:40.122755Z",
     "start_time": "2021-07-28T15:51:40.118525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dima/ViTransformer/checkpoints/cifar10-epoch=0-val_loss=1.841.ckpt'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a772a7d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T15:49:06.744068Z",
     "start_time": "2021-07-28T15:49:06.580061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaViT(\n",
       "  (embedding): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 512, kernel_size=(8, 8), stride=(8, 8))\n",
       "    (norm): Identity()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (train_acc): Accuracy()\n",
       "  (val_acc): Accuracy()\n",
       "  (test_acc): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_from_checkpoint(model_checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5782700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T08:51:36.229300Z",
     "start_time": "2021-07-29T08:51:36.106813Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/.cache/pypoetry/virtualenvs/vitransformer-feYZhwM1-py3.8/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:508: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "/home/dima/.cache/pypoetry/virtualenvs/vitransformer-feYZhwM1-py3.8/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = nn.Parameter(torch.empty(4,4))\n",
    "optimizer = torch.optim.Adam([p], lr=1e-3)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 2000, 1e-4)\n",
    "# lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
    "\n",
    "# Plotting\n",
    "epochs = list(range(4000))\n",
    "lr = []\n",
    "for _ in range(4000):\n",
    "    lr.append(lr_scheduler.get_lr())\n",
    "    lr_scheduler.step()\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(epochs, lr)\n",
    "plt.ylabel(\"Learning rate factor\")\n",
    "plt.xlabel(\"Iterations (in batches)\")\n",
    "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c73064a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T19:39:13.338974Z",
     "start_time": "2021-07-27T19:39:13.326186Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, num_channels=3, emb_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size\"\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb8a7e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T19:35:39.412491Z",
     "start_time": "2021-07-27T19:35:39.406029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "patch_embed = PatchEmbed(img_size=32, patch_size=8, embed_dim=512)\n",
    "patches = patch_embed(images[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85372d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10aef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n",
    "        - https://arxiv.org/abs/2012.12877\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None, weight_init=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            weight_init: (str): weight init scheme\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.dist_token is not None:\n",
    "            trunc_normal_(self.dist_token, std=.02)\n",
    "        if mode.startswith('jax'):\n",
    "            # leave cls token as zeros to match jax impl\n",
    "            named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)\n",
    "        else:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "            self.apply(_init_vit_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        _init_vit_weights(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        if self.dist_token is None:\n",
    "            return self.head\n",
    "        else:\n",
    "            return self.head, self.head_dist\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        if self.num_tokens == 2:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):\n",
    "    \"\"\" ViT weight initialization\n",
    "    * When called without n, head_bias, jax_impl args it will behave exactly the same\n",
    "      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n",
    "    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if name.startswith('head'):\n",
    "            nn.init.zeros_(module.weight)\n",
    "            nn.init.constant_(module.bias, head_bias)\n",
    "        elif name.startswith('pre_logits'):\n",
    "            lecun_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            if jax_impl:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    if 'mlp' in name:\n",
    "                        nn.init.normal_(module.bias, std=1e-6)\n",
    "                    else:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "            else:\n",
    "                trunc_normal_(module.weight, std=.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    elif jax_impl and isinstance(module, nn.Conv2d):\n",
    "        # NOTE conv was left to pytorch default in my original init\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
